# Lecture: Caches and VM

### What Is A Cache?
- motivation behind caches is to avoid accessing data directly from the memory as it is slow
- a direct-mapped cache has one entry per page (does not provide good uniform distribution of data)
- a cache is a small memory that is in-between the CPU and memory
- before doing a lookup in memory, data is looked up in the cache
  - input to the cache is an address that would like to be looked at
  - cache will return data as well a signal 'hint' if the data can be trusted
  - if the signal 'hint' says the data cannot be trusted, then memory needs to be accessed to get the most up-to-date value and possibly update the cache

### Implementing Caches
- ``k = 2^10``, ``b`` = bit, ``B`` = byte
- how many index bits are need to represent a cache file that allows to save 1k entries?
  - ``10`` (i.e. 10 bits are needed to represent 1k unique values)
- what is a good indexing function?
  - uses address bits from the ``lsb`` (least significant bits) end which provide a better distribution of values (are more likely to change rapidly throughout the execution)
- hit: use the data provided from the cache
- not-hit: use data from memory and possibly store it in the cache
- given a 32 bit input, with its ``msb`` and ``lsb``, how to implement a direct-mapped cache:
  - toss away the last 2 ``lsb``
  - use next 10 ``lsb`` for the indexing function
  - read the value stored in cache identified by those 10 ``lsb``
  - compare 20 bits in the address tag with the part of the identifier that was not use for the indexing function
  - a valid bit that says if the space in the cache has been initialized or not
  - if the comparison is true and the valid bit is set, then it produces a single hit signal and the data stored in that location is sent back to the processor
  - memory overhead (how many useful bits v.s. how much extra-overhead): ``22/32 = 66%``
  - latency: SRAM + CMP + AND (latency it takes to make a SRAM access + time it takes to make comparison + time it takes to go through the AND gate)

### Judging Caches
- architecture is usually measured in terms of
  - CPI = cycles per instruction
  - IPC = instructions per cycle (= ``1/CPI ``)
- cache performance parameters
  - cache 'hit ratio' %
  - cache 'miss ratio' % (1 - cache 'hit ratio')
  - hit time (CPU cycles)
  - miss time (CPU cycles)
  - memory ratio % (fraction of instructions to access memory)
  - ...
- ``CPI = 1 - mem_ratio +
        mem_ratio * (hit_ratio * hit_time) +
        mem_ratio * (1 - hit_ratio) * miss_time``
- how get more effective caches?
  - larger cache (more capacity)
  - cache block size (larger cache lines)
  - more placement choice (more associativity)
  - innovative caches (victim, skewed, ...)
  - cache hierarchies (L1, L2, L3, CMR)
  - latency-hiding (weaker memory models)
  - latency-avoiding (pre-fetching)
  - cache avoiding (cache bypass)
  - optimized application/compiler
  - ...
- why do you miss in a cache? (three C's)
  - compulsory miss (touching data for the first time)
  - capacity miss (the cache is too small)
  - conflict misses (non-ideal cache implementation)

### Cache Size and Lines
- cache size refers to the size of the data it can the cache can store
- a larger cache size can help avoiding capacity misses
- cache line is the smallest unit of memory than can be transferred between the main memory and the cache
- temporal locality refers to the likelihood of accessing the same data again soon
- spatial locality refers to the likelihood of accessing nearby data soon
- spatial locality is explored by large cache lines
- using a larger cache line can help avoiding compulsory misses
- for a small cache a smaller cache line is better
- for a large cache a larger cache line is better

### Associativity
- two-way associative cache
  - cache is made up of sets that can fit two blocks each
  - the index is used to find the set, and the tag helps find the block within the set
- pros
  - avoid conflict misses
- cons
  - slower access time
  - more complex implementation comparators, muxes, ...
  - higher dynamic power consumption
- fully associative cache
  - no index is needed since a cache block can go anywhere in the cache
  - every tag must be compared when finding a block in the cache, but block placement is very flexible
  - very expensive
  - only used for small caches
- associative caches remove conflict misses

### Optimizing Caches
- how to choose which cache to replace?
  - least-recently used (LRU) algorithm
    - throw out the longest unused cache line
    - requires to keep more information to maintain a history about the sets
    - implementing a true LRU replacement for highly associative caches is expensive
  - not most recently used
    - remember who used it last
  - pseudo-LRU
    - based on course time stamps
  - random replacement
- handling dirty cache lines
  - write-back caches
    - a 'dirty bit' per cache line indicates an altered cache line
    - write data back to memory at replacement, called write-back
  - write-through
    - always write through to memory
    - data will never by dirty
    - no write-backs
- power consumption
  - static power or leakage power
    - current leaks through transistors
    - proportional to the number of transistors used
    - proportional to cache capacity
  - dynamic power
    - extra energy needed to read SRAMs bits
    - proportional to the number of SRAM bits read
  - which power dominates?
    - associative and fast first-level cache (L1) cache: dynamic
    - large slower cache last-level cache (LLC): static
- L1 caches are implemented as fast-cache and sometimes have smaller cache lines than L2
- L3 caches consumption is dominated by static power and are often shared by many CPUs (cores)
- the hardware pre-fetcher tries to anticipate what data is going to be accessed next
  - improves memory-level parallelism MLP
- sequential pre-fetching
  - sequential streams to a  page
  - some number of pre-fetch streams supported
  - often only for L2 and L3
- PC-based pre-fetching
  - detects strides from te same PC
  - often for L1 caches
- adjacency pre-fetching
  - on a miss, also bring in the neighboring cache line
  - often only for L2 and L3
- cache line: data chunk move to/from a cache
- cache set: fraction of the cache identified by the index
- associativity: number of alternative storage places for a cache line
- replacement policy: picking the victim to throw out from a set (LRU/Random/Nehalem)
- temporal locality: likelihood to access the same data again soon
- spatial locality: likelihood to access nearby data again soon
- how much to replicate data among the different cache levels?
  - inclusive
    - install a copy of the data in L1 and keep the data in L2
    - when the L2 data is replaced, force eviction from L1
    - if the data is not in L2, it is not in L1 either
  - non-inclusive
    - install a copy of the data in L1 and keep the data in L2
    - when the L2 data is replaced, L1 data survives
    - if the data is not in L2, it may be in L1
  - exclusive
    - move the data from L2 to L1
    - at L1 replacement, move the data back to L2
    - data is either in L1 or L2, but never in both
- a victim cache can help reduce the number of conflict misses
- sub-blocking lowers the memory overhead and avoids problems with false sharing
- sub-blocking will not explore as much spatial locality, and still poor utilization of SRAM (fewer sparse 'things' allocated)
- skewed-associative cache reads fewer bits from the SRAM on a cache lookup (dynamic energy)
