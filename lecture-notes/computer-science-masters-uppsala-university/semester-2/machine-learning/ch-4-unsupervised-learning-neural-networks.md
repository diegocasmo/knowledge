# Lecture: Unsupervised Learning Neural Networks

Readings: Ch 4, Computational Intelligence: An Introduction, Andries P. Engelbrecht

### Competitive Learning
- classification without telling which data belong to which class.
- requires that class membership can be decided by structural features in the data, and that the learning system can find these features
- K–means
  - sample (at random) ``K`` vectors from the data as initial values of ``c1, c2, ... c_K``
  - split the data in ``K`` subsets, ``D1, D2, ..., D_K``, where ``D1`` is the set of all points with ``c1`` as their closest codebook vector, ``D2`` is the corresponding set for ``c2``, etc
  - move ``c1`` towards the mean in ``D1``, ``c2`` towards the mean in ``D2``, etc
  - repeat from until convergence (until the codebook vectors stop moving)
- Voronoi regions
  - K-means define Voronoi regions in the input space
  - the Voronoi region around a codebook vector ``ci`` is the region in which ``ci`` would be the closest codebook vector
- Competitive Learning (LVQ-1 without neighborhood function)
  - the goal is to classify ``N``-dimensional data into ``M`` classes
  - the network consists of ``M`` nodes, fully connected to the inputs
  - the weight vector is a  coordinate in the N-dimensional input space (each node has a position)
  - changing the weights = moving the node
  - usual interpretation
    - given input vector, ``x``, find the node ``k`` which is closest to ``x`` (the winner, the node with the smallest distance between its weight vector and the input vector)
    - update node ``k`` so that it becomes even more likely to win next time the same input vector shows up (move it towards the input vector)
  - neural interpretation
    - find the node with the greatest weighted sum
    - update the weights of node ``k`` to increase the weighted sum for ``x`` (all other weights are left unchanged)
  - standard competitive learning rule
    - ``∆wk = η(x−wk)`` = ``η(x_i − w_{ki})`` ,``1≤i≤N``
    - only the winner is moved
  - CL with bad (random) initialization
    - nodes which are never used (no data in that Voronoi region)
    - nodes which cover more than one cluster, and therefore moves back and forth between them
  - CL initialized from the data
    - better coverage, all nodes are used
    - but still not optimal, unless we're very lucky (bad distribution of nodes)
- competitive learning + batch learning = K-means
- the winner-takes-all problem: if a single node wins a lot, it may become invincible
  - solutions
    - initialize the network by drawing vectors from the training set (as in K-means)
    - modify the distance measure to include the frequency of winning

### Dimensionality Reduction
- project high dimensional data into some sub-space of lower dimension without destroying the class information in the data
- Principal Component Analysis (PCA)
  - find ``M`` orthogonal vectors in the input space, in the directions of greatest variance
  - project the data down to those vectors
  - principal components are the eigenvectors of the correlation matrix of the input data, corresponding to the ``M`` largest eigenvalues
- Self Organizing Feature Maps (SOFM)
  - non-linear dimensionality reduction method which preserves the topological structure of the input space
  - nodes are organized in a two-dimensional grid
  - extension of competitive learning to update not only the winner, ``k``, but also its closest neighbors (on the grid)
  - we don't want to update the neighbors as much as the winner
    - define a neighborhood function ``f(j, k)`` which is 1 for the winner (``j = k``) and then decreases with the distance (on the grid) from node ``k`` (i.e. Gaussian)
  - learning rule
    - ``∆w_{ji} = ηf(j,k)(x_i − w_{ji})`` (for all nodes ``j`` and all inputs ``i``)
  - SOFM is often trained in two phases
    - ordering phase
      - to find the number of classes and their approximate locations
      - start with a large ``η`` and a large neighborhood
    - convergence phase
      - to decide the exact location and form of the classes
      - start with low value of ``η``
      - about ``10x`` longer training time than in the ordering phase
  - SOFM forms a density function of the data. For example, uniformly distributed input data leads to uniformly distributed weight vectors

### Growing Neural Gas (GNG)
- unsupervised growing algorithm for clustering
- dynamic size (a growing/shrinking algorithm)
  - though it grows faster than it shrinks
- dynamic neighborhood (who is neighbor to whom is not fixed)
  - defined by a graph
  - not weighted connections, just edges in a graph
- all parameters are constant
  - great for on-line learning and for following moving targets
- which nodes to move (given an input), and by how much?
  - move not only the winner (``k``), but also its (current) neighbors
  - the winner is moved using a much greater gain factor than the neighbors (``εk >>εn``)
    - ``∆wk = εk(x−wk)``
    - ``∆wn = εn(x−wn)``
    - where ``k`` is the winner, ``n`` is a neighbor, ``x`` is the input vector and ``w`` is the node position (its weight vector)
    - both gain factors are constant
- how to define and update the neighborhood graph?
  - all current neighbors are connected by edges in a graph
  - each edge has an associated age
  - for each input vector
    - find the the closest node (``k``, the winner), and the second closest (``r``)
    - if ``k`` and ``r`` are not already connected (neighbors), connect them with a new edge
    - set the age of the edge between ``k`` and ``r`` to 0
    - the age of all other edges from ``k`` is incremented by one
      - if any edge becomes too old (``> a_max``), remove it
      - if any node loses its last edge this way, remove that node as well
- how to grow (when and where should we insert new nodes)?
  - every time a winner, ``k``, is found, add the distance (from the input) to a local error variable
    - error is proportional to the accumulated distance this node has moved, as a winner
  - at fixed time intervals, insert a new node where it is most likely needed
    - halfway between the node with the largest error, and the node among its current neighbors with the largest error
  - the error of a node is decayed over time
- extension: GNG-U: removes nodes with low utility, based on frequency of winning and closeness to other nodes
- GNG is used mostly for modeling distributions
  - can be used to train the hidden nodes in RBF networks
